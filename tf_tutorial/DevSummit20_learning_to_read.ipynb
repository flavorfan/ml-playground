{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Understanding Encoder-Decoder Sequence to Sequence Model   \n",
    "https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T09:26:57.322875Z",
     "start_time": "2020-03-29T09:26:54.471322Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T09:54:23.795006Z",
     "start_time": "2020-03-29T09:54:23.779004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D has no label.\n",
      " Volume Serial Number is 7AB0-BEFF\n",
      "\n",
      " Directory of D:\\work\\work_git\\ml-playground\n",
      "\n",
      "03/29/2020  05:52 PM    <DIR>          .\n",
      "03/29/2020  05:52 PM    <DIR>          ..\n",
      "03/18/2020  09:34 PM    <DIR>          autoencoder\n",
      "03/29/2020  05:52 PM    <DIR>          dataset\n",
      "03/16/2020  09:59 PM    <DIR>          lecture_homework\n",
      "03/11/2020  06:11 PM             4,514 load_test_autoencoder.py\n",
      "03/18/2020  09:01 PM    <DIR>          logs\n",
      "03/18/2020  11:53 PM    <DIR>          output\n",
      "03/18/2020  08:06 PM             2,377 README.md\n",
      "03/10/2020  06:19 PM             5,660 restore_training.py\n",
      "03/16/2020  09:59 PM    <DIR>          tf_dataset\n",
      "03/29/2020  05:43 PM    <DIR>          tf_tutorial\n",
      "03/16/2020  09:59 PM    <DIR>          training_plot\n",
      "03/09/2020  10:18 PM             2,998 train_conv_autoencoder.py\n",
      "03/10/2020  06:19 PM             6,703 train_denoising_autoencoder.py\n",
      "03/18/2020  08:06 PM             5,907 train_fan_autoencoder.py\n",
      "03/19/2020  09:38 PM             9,870 train_on_mnist_dset.py\n",
      "03/18/2020  08:06 PM             4,259 train_vae.py\n",
      "03/16/2020  09:59 PM    <DIR>          utils\n",
      "03/18/2020  08:48 PM    <DIR>          visualization\n",
      "03/09/2020  10:18 PM                 0 __init__.py\n",
      "               9 File(s)         42,288 bytes\n",
      "              12 Dir(s)  108,760,580,096 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir ..\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:07:51.253931Z",
     "start_time": "2020-03-29T10:07:51.247929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from a dataset comprising lines \n",
    "# from one or more text files.\n",
    "lines = tf.data.TextLineDataset('..\\dataset\\CBTest\\data\\cbt_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:07:52.507223Z",
     "start_time": "2020-03-29T10:07:52.500221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'_BOOK_TITLE_ : Andrew_Lang___Prince_Prigio.txt.out', shape=(), dtype=string)\n",
      "tf.Tensor(b'CHAPTER I. -LCB- Chapter heading picture : p1.jpg -RCB- How the Fairies were not Invited to Court .', shape=(), dtype=string)\n",
      "tf.Tensor(b'Once upon a time there reigned in Pantouflia a king and a queen .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in lines.take(3):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T09:57:02.489872Z",
     "start_time": "2020-03-29T09:57:02.482882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:07:59.064090Z",
     "start_time": "2020-03-29T10:07:59.061089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter Out Title Lines First \n",
    "def is_title(x):\n",
    "    return tf.strings.regex_full_match(x, \"_BOOK_TITLE_.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:08:00.768473Z",
     "start_time": "2020-03-29T10:08:00.764473Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then We Remove All Punctuation\n",
    "punctuation = r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']'\n",
    "def remove_punc(x):\n",
    "    return tf.strings.regex_replace(x, punctuation, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:12:57.680401Z",
     "start_time": "2020-03-29T10:12:57.677400Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_extra_spaces(x):\n",
    "    return tf.strings.join(tf.strings.split(x), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:32:53.183245Z",
     "start_time": "2020-03-29T10:32:53.181245Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_lower(x):\n",
    "    return tf.strings.lower(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:08:21.429580Z",
     "start_time": "2020-03-29T10:08:21.378569Z"
    }
   },
   "outputs": [],
   "source": [
    "lines = lines.filter(lambda x: not is_title(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:08:22.573839Z",
     "start_time": "2020-03-29T10:08:22.564836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'CHAPTER I. -LCB- Chapter heading picture : p1.jpg -RCB- How the Fairies were not Invited to Court .', shape=(), dtype=string)\n",
      "tf.Tensor(b'Once upon a time there reigned in Pantouflia a king and a queen .', shape=(), dtype=string)\n",
      "tf.Tensor(b'With almost everything else to make them happy , they wanted one thing : they had no children .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for line in lines.take(3):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:32:56.362006Z",
     "start_time": "2020-03-29T10:32:56.347002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'CHAPTER I   LCB  Chapter heading picture   p1 jpg  RCB  How the Fairies were not Invited to Court  ', shape=(), dtype=string)\n",
      "tf.Tensor(b'Once upon a time there reigned in Pantouflia a king and a queen  ', shape=(), dtype=string)\n",
      "tf.Tensor(b'With almost everything else to make them happy   they wanted one thing   they had no children  ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "lines = lines.map(lambda x: remove_punc(x))\n",
    "for line in lines.take(3):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:31:23.249654Z",
     "start_time": "2020-03-29T10:31:23.247653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Then We Remove All Extra Spaces Created By The Previous FN\n",
    "# lines = lines.map(lambda x: remove_extra_spaces(x))\n",
    "# for line in lines.take(3):\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:32:59.869489Z",
     "start_time": "2020-03-29T10:32:59.826480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'chapter i   lcb  chapter heading picture   p1 jpg  rcb  how the fairies were not invited to court  ', shape=(), dtype=string)\n",
      "tf.Tensor(b'once upon a time there reigned in pantouflia a king and a queen  ', shape=(), dtype=string)\n",
      "tf.Tensor(b'with almost everything else to make them happy   they wanted one thing   they had no children  ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Then We Turn All The Uppercase Letters into Lowercase Letters\n",
    "lines = lines.map(lambda x: make_lower(x))\n",
    "for line in lines.take(3):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:34:15.802008Z",
     "start_time": "2020-03-29T10:34:15.748997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'chapter' b'i' b'lcb' b'chapter' b'heading' b'picture' b'p1' b'jpg'\n",
      " b'rcb' b'how' b'the'], shape=(11,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'fairies' b'were' b'not' b'invited' b'to' b'court' b'once' b'upon' b'a'\n",
      " b'time' b'there'], shape=(11,), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'reigned' b'in' b'pantouflia' b'a' b'king' b'and' b'a' b'queen' b'with'\n",
      " b'almost' b'everything'], shape=(11,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Get words from lines\n",
    "words = lines.map(tf.strings.split)\n",
    "wordsets = words.unbatch().batch(11)\n",
    "\n",
    "for row in wordsets.take(3):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:43:41.717506Z",
     "start_time": "2020-03-29T10:43:41.714508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None,), types: tf.string>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:19.320436Z",
     "start_time": "2020-03-29T11:14:19.315435Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_example_label(row):\n",
    "    example = tf.strings.reduce_join(row[:-1], separator=' ')\n",
    "#     print(example)\n",
    "    example = tf.expand_dims(example, axis=0)\n",
    "#     print(example)\n",
    "    label = row[-1:]\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:20.177089Z",
     "start_time": "2020-03-29T11:14:20.154085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: tf.Tensor(\n",
      "[b'chapter' b'i' b'lcb' b'chapter' b'heading' b'picture' b'p1' b'jpg'\n",
      " b'rcb' b'how' b'the'], shape=(11,), dtype=string)\n",
      "exmaple: tf.Tensor([b'chapter i lcb chapter heading picture p1 jpg rcb how'], shape=(1,), dtype=string)\n",
      "label tf.Tensor([b'the'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for row in wordsets.take(1):\n",
    "    print('row:',row)\n",
    "    example, label = get_example_label(row)\n",
    "    print('exmaple:',example)\n",
    "    print('label',label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:33.684640Z",
     "start_time": "2020-03-29T11:14:33.624627Z"
    }
   },
   "outputs": [],
   "source": [
    "# get_example_label is a simple fn to split wordsets into examples and labels\n",
    "# First ten words are the example and last word is the label\n",
    "data = wordsets.map(get_example_label)\n",
    "data = data.shuffle(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:34.694494Z",
     "start_time": "2020-03-29T11:14:34.490103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
      "array([b'majesty said prigio you must permit me to correct your'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'policy'], dtype=object)>)\n"
     ]
    }
   ],
   "source": [
    "for row in data.take(1):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:37.998920Z",
     "start_time": "2020-03-29T11:14:37.993933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((1,), (None,)), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproces the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:44:59.986771Z",
     "start_time": "2020-03-29T10:44:59.983650Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:45:58.582769Z",
     "start_time": "2020-03-29T10:45:58.574756Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_sequence_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:56:44.213825Z",
     "start_time": "2020-03-29T10:56:15.951910Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(lines.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:57:05.038804Z",
     "start_time": "2020-03-29T10:57:05.030802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'the', b'and', b'to', b'a', b'of']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T10:57:32.410719Z",
     "start_time": "2020-03-29T10:57:32.403718Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'repay', b'rejoice', b'reasonable', b'rail', b'quoted']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:14:59.137802Z",
     "start_time": "2020-03-29T11:14:58.916405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[b'suit in a moment these were provided he bathed dressed']\n",
      " [b'he would have liked to say do n t you']\n",
      " [b'they were lying then he ran downstairs and walked out']], shape=(3, 1), dtype=string)\n",
      "tf.Tensor(\n",
      "[[1102   11    5  280  210   43 4758    8    1  942]\n",
      " [   8   45   35  523    4  138   37   30   28   13]\n",
      " [  31   43  767   58    8  301 2451    3  442   47]], shape=(3, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for batch in data.batch(3).take(1):\n",
    "    print(batch[0])\n",
    "    print(vectorize_layer(batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:19:49.387875Z",
     "start_time": "2020-03-29T11:19:46.199401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting tensorflow_addons\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a9/4a/bdbca16d6f29dd53ea1cdf2501c5d1a844fed716392757fe8ae397ba2ac6/tensorflow_addons-0.8.3-cp36-cp36m-win_amd64.whl (837 kB)\n",
      "Collecting typeguard\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/06/37/d236aec27f8a8eed66f1a17116eb51684528cf8005a6883f879fe2e842ae/typeguard-2.7.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: typeguard, tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.8.3 typeguard-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:19:54.688248Z",
     "start_time": "2020-03-29T11:19:54.625234Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:20:05.983493Z",
     "start_time": "2020-03-29T11:20:05.980492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.3\n"
     ]
    }
   ],
   "source": [
    "print(tfa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T11:20:27.740177Z",
     "start_time": "2020-03-29T11:20:27.736176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AttentionMechanism', 'AttentionWrapper', 'AttentionWrapperState', 'BahdanauAttention', 'BahdanauMonotonicAttention', 'BaseDecoder', 'BasicDecoder', 'BasicDecoderOutput', 'BeamSearchDecoder', 'BeamSearchDecoderOutput', 'BeamSearchDecoderState', 'CustomSampler', 'Decoder', 'FinalBeamSearchDecoderOutput', 'GreedyEmbeddingSampler', 'InferenceSampler', 'LuongAttention', 'LuongMonotonicAttention', 'SampleEmbeddingSampler', 'Sampler', 'ScheduledEmbeddingTrainingSampler', 'ScheduledOutputTrainingSampler', 'SequenceLoss', 'TrainingSampler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'attention_wrapper', 'basic_decoder', 'beam_search_decoder', 'decoder', 'dynamic_decode', 'gather_tree', 'gather_tree_from_array', 'hardmax', 'loss', 'monotonic_attention', 'safe_cumprod', 'sampler', 'sequence_loss', 'tile_batch']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tfa.seq2seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T12:13:31.404779Z",
     "start_time": "2020-03-29T12:13:31.391775Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 5000\n",
    "WINDOW_LENGTH = 11\n",
    "class EncoderDecoder(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 max_features=MAX_VOCAB_SIZE, \n",
    "                 output_seq_len=WINDOW_LENGTH-1,  ## \n",
    "                 embedding_dims=200, \n",
    "                 rnn_units=512):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_features = max_features\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.rnn_units = rnn_units\n",
    "        \n",
    "        # vecctorize\n",
    "        self.vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "                max_tokens=max_features,\n",
    "                output_sequence_length=10)\n",
    "        # encoder\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(\n",
    "            max_features + 1, embedding_dims)\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(rnn_units, return_state=True)\n",
    "        # decoder \n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(\n",
    "            max_features + 1, embedding_dims)\n",
    "        \n",
    "        \n",
    "        # ---------------- MAYBE NOT NECESSARY ----------------\n",
    "        # Sampler (for use during training)\n",
    "        # This was not shown during the talk but it is pretty obvious\n",
    "        sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        \n",
    "        # ref the https://github.com/tensorflow/addons/issues/603\n",
    "#         train_sampler = tfa.seq2seq.TrainingSampler()\n",
    "#         inference_sampler = tfa.seq2seq.InferenceSampler()\n",
    "        \n",
    "        # This was not shown during the talk but is required... \n",
    "        # This is my best guess\n",
    "        decoder_cell = tf.keras.layers.LSTMCell(units=self.rnn_units)\n",
    "        # ---------------- MAYBE NOT NECESSARY ----------------\n",
    "        \n",
    "        \n",
    "        projection_layer = tf.keras.layers.Dense(max_features)\n",
    "        \n",
    "        \n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(\n",
    "            decoder_cell, sampler, output_layer=projection_layer)\n",
    "\n",
    "#         self._train_decoder = tfa.seq2seq.BasicDecoder(\n",
    "#                 decoder_cell, train_sampler , output_layer=projection_layer)\n",
    "    \n",
    "#         self._inference_decoder = tfa.seq2seq.BasicDecoder(\n",
    "#                 decoder_cell, inference_sampler, output_layer=projection_layer)\n",
    "        \n",
    "        # pay attention\n",
    "        self. attention = tf.keras.layers.Attention()\n",
    "        \n",
    "#     def call(inputs, training=None):\n",
    "#         if training is None:\n",
    "#             training = tf.keras.backend.learning_phase()\n",
    "        \n",
    "#         if training:\n",
    "#             # training stuff\n",
    "#             pass\n",
    "#         else:\n",
    "#             # inference stuff\n",
    "#             pass\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \"\"\" Overwrite built-in train_step method\n",
    "\n",
    "        Args:\n",
    "            data (tuple): The example (ten `words`), and the label (one `word`)\n",
    "\n",
    "        Returns:\n",
    "            Metric results for all passed metrics\n",
    "        \"\"\"\n",
    "\n",
    "        # Split data into example (x) and label (y)\n",
    "        x, y = data[0], data[1]\n",
    "\n",
    "        # Vectorize the example words (x)\n",
    "        x = self.vectorize_layer(x)\n",
    "\n",
    "        # Vectorize the labels\n",
    "        # This will by default pad the output to 10 ... but we only need the\n",
    "        # first entry (the true label not the useless padding)\n",
    "        y = self.vectorize_layer(y)[:, 0:1]\n",
    "\n",
    "        # Convert our label into a one-hot encoding based on the max number of\n",
    "        # features that we will be using for our model\n",
    "        y_one_hot = tf.one_hot(y, self.max_features)\n",
    "\n",
    "        # Everything within GradientTape is recorded \n",
    "        # for later automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # --- <ENCODER STUFF> ---\n",
    "\n",
    "            # Transform the example utilizing the encoder embedding\n",
    "            inputs = self.encoder_embedding(x)\n",
    "\n",
    "            # Get the encoder outputs and state by \n",
    "            # utilizing the encoder (lstm_layer)\n",
    "            #   - encoder_outputs : [max_time, batch_size, num_units]\n",
    "            #   - encoder_state   : [state_h, state_c]\n",
    "            #       * state_h --- The Hidden State\n",
    "            #       * state_c --- The Cell   State\n",
    "            encoder_outputs, state_h, state_c = self.lstm_layer(inputs)\n",
    "\n",
    "            # --- </ENCODER STUFF> ---\n",
    "\n",
    "            # --- <ATTN STUFF> ---\n",
    "\n",
    "            # Pass the encoder outputs and hidden state allowing us\n",
    "            # to track the intermediate state coming out of the encoder layers\n",
    "            attn_output = self.attention([encoder_outputs, state_h])\n",
    "            attn_output = tf.expand_dims(attn_output, axis=1)\n",
    "\n",
    "            # --- </ATTN STUFF> ---\n",
    "\n",
    "            # --- <DECODER STUFF> ---\n",
    "\n",
    "            # ??? Create an empty embedding ???\n",
    "            targets = self.decoder_embedding(tf.zeros_like(y))\n",
    "\n",
    "            # Concat the output of the attention layer to the last axis\n",
    "            # of the empty targets embedding\n",
    "            concat_output = tf.concat([targets, attn_output], axis=-1)\n",
    "\n",
    "            # Predict the targets using the state from the encoder\n",
    "            outputs, _, _ = \\\n",
    "                self.decoder(concat_output, initial_state=[state_h, state_c])\n",
    "\n",
    "            # --- </DECODER STUFF> ---\n",
    "            # ! add by fanck\n",
    "            y_pred = outputs.rnn_output      \n",
    "            loss = self.compiled_loss(\n",
    "              y_one_hot, \n",
    "              y_pred, \n",
    "              regularization_losses=self.losses)\n",
    "\n",
    "        # Automatically differeniate utilizing the loss and trainable variables\n",
    "        trainable_variables = self.trainable_variables  # add by fanck\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "        # Collect the outputs so that they can be optimized\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        # Update the metric state prior to return\n",
    "        self.compiled_metrics.update_state(y_one_hot, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def predict_step(self, data, select_from_top_n=1):\n",
    "        x = data\n",
    "        if isinstance(x, tuple) and len(x) == 2:\n",
    "            x = x[0]\n",
    "        x = self.vectorize_layer(x)\n",
    "        embedded_inputs = self.encoder_embedding(x)\n",
    "        encoder_outputs, state_h, state_c = self.lstm_layer(embedded_inputs)\n",
    "        attn_output = self.attention([encoder_outputs, state_h])\n",
    "        attn_output = tf.expand_dims(attn_output, axis=1)\n",
    "\n",
    "        targets = self.decoder_embedding(tf.zeros_like(x[:, -1:]))\n",
    "        concat_output = tf.concat([targets, attn_output], axis=-1)\n",
    "        outputs, _, _ = self.decoder(\n",
    "            concat_output, initial_state=[state_h, state_c])\n",
    "\n",
    "        y_pred = tf.squeeze(outputs.rnn_output, axis=1)\n",
    "        top_n = tf.argsort(\n",
    "            y_pred[:, 2:], axis=1, direction='DESCENDING')[: ,:select_from_top_n]\n",
    "        chosen_indices = tf.random.uniform(\n",
    "            [top_n.shape[0], 1], minval=0, maxval=select_from_top_n, \n",
    "            dtype=tf.dtypes.int32)\n",
    "        counter = tf.expand_dims(tf.range(0, top_n.shape[0]), axis=1)\n",
    "        indices = tf.concat([counter, chosen_indices], axis=1)\n",
    "        choices = tf.gather_nd(top_n, indices)\n",
    "        words = [self.vectorize_layer.get_vocabulary()[i] for i in choices]\n",
    "        return words\n",
    "\n",
    "    def predict(self, starting_string, num_steps=50, select_from_top_n=1):\n",
    "        s = tf.compat.as_bytes(starting_string).split(b' ')\n",
    "        for _ in range(num_steps):\n",
    "            windowed = [b' '.join(s[-10:])]\n",
    "            pred = self.predict_step([windowed], select_from_top_n=select_from_top_n)\n",
    "            s.append(pred[0])\n",
    "        return b' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T12:16:29.036532Z",
     "start_time": "2020-03-29T12:16:00.767632Z"
    }
   },
   "outputs": [],
   "source": [
    "model = EncoderDecoder()\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "model.vectorize_layer.adapt(lines.batch(256))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T12:18:24.856700Z",
     "start_time": "2020-03-29T12:18:24.853699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((1,), (None,)), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-29T12:17:10.023496Z",
     "start_time": "2020-03-29T12:17:09.780442Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-8abd91917004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m model.fit(data.batch(256),\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m45\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='text_gen')])\n\u001b[0m",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, standardize_function, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstandardize_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;31m# Note that the dataset instance is immutable, its fine to reusing the user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mstandardize_function\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_weight_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sample_weight_mode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m       \u001b[0mstandardize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m       \u001b[1;31m# Then we map using only the tensor standardization portion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2358\u001b[0m     \u001b[0mis_compile_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2360\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compile_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2361\u001b[0m       \u001b[0mis_compile_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_compile_from_inputs\u001b[1;34m(self, all_inputs, target, orig_inputs, orig_target)\u001b[0m\n\u001b[0;32m   2578\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2579\u001b[0m         target = training_utils.cast_if_floating_dtype_and_mismatch(\n\u001b[1;32m-> 2580\u001b[1;33m             target, self.outputs)\n\u001b[0m\u001b[0;32m   2581\u001b[0m       training_utils.validate_input_types(target, orig_target,\n\u001b[0;32m   2582\u001b[0m                                           allow_dict=False, field_name='target')\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcast_if_floating_dtype_and_mismatch\u001b[1;34m(targets, outputs)\u001b[0m\n\u001b[0;32m   1334\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m     \u001b[1;31m# There is one target, so output[0] should be the only output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcast_single_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m   \u001b[0mnew_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# ERROR OCCURS ON THIS LINE\n",
    "model.fit(data.batch(256),\n",
    "          epochs=45,\n",
    "          callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='text_gen')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
